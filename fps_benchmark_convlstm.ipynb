{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4152b0f-ad46-4438-ab02-0670f320d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 1: GPU Оценка производительности\n",
      "----------------------------------------\n",
      "Устройство: GPU: NVIDIA GeForce RTX 5090 (31.4 GB)\n",
      "Загрузка модели.\n",
      "Прогрев GPU.\n",
      "Измерение производительности (1000 тестов)...\n",
      "\n",
      "GPU Результаты:\n",
      "  Устройство: GPU: NVIDIA GeForce RTX 5090 (31.4 GB)\n",
      "  Inference Latency: 22.2 ms\n",
      "  Sequence Processing Rate: 45.1 sequences/sec\n",
      "  Frame Throughput: 2255 frames/sec\n",
      "\n",
      "Phase 2: CPU Оценка производительности\n",
      "----------------------------------------\n",
      "NOTE: CPU benchmark может занять несколько минут.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Продолжить с CPU? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Принудительно используется CPU\n",
      "Устройство: CPU: 64 cores\n",
      "Загрузка модели.\n",
      "Минимальная инициализация CPU.\n",
      "Измерение производительности (10 тестов)...\n",
      "  Тест 1/10...\n",
      "    Время: 1402.4 мс\n",
      "  Тест 2/10...\n",
      "    Время: 1398.8 мс\n",
      "  Тест 3/10...\n",
      "    Время: 1386.8 мс\n",
      "  Тест 4/10...\n",
      "    Время: 1390.3 мс\n",
      "  Тест 5/10...\n",
      "    Время: 1389.9 мс\n",
      "  Тест 6/10...\n",
      "    Время: 1397.0 мс\n",
      "  Тест 7/10...\n",
      "    Время: 1466.6 мс\n",
      "  Тест 8/10...\n",
      "    Время: 1468.3 мс\n",
      "  Тест 9/10...\n",
      "    Время: 1391.0 мс\n",
      "  Тест 10/10...\n",
      "    Время: 1403.9 мс\n",
      "\n",
      "CPU Результаты:\n",
      "  Устройство: CPU: 64 cores\n",
      "  Inference Latency: 1409.5 ms\n",
      "  Sequence Processing Rate: 0.7 sequences/sec\n",
      "  Frame Throughput: 35 frames/sec\n",
      "\n",
      "======================================================================\n",
      "Сравнение производительности.\n",
      "======================================================================\n",
      "Метрики                   GPU             CPU            \n",
      "----------------------------------------------------------------------\n",
      "Inference Latency (ms)    22.2            1409.5         \n",
      "Sequence FPS              45.1            0.7            \n",
      "Frame Throughput          2255            35             \n",
      "\n",
      "1. torch.no_grad() - отключение автограда\n",
      "2. model.eval() - отключение dropout/batchnorm\n",
      "3. torch.cuda.synchronize() - синхронизация GPU\n",
      "4. Прогрев GPU перед измерениями\n",
      "5. Множественные измерения для статистики\n",
      "6. perf_counter() - точное измерение времени\n"
     ]
    }
   ],
   "source": [
    "from model.model import YOLOBackboneConvLSTM\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def benchmark(force_cpu=False):\n",
    "    # Выбор устройства\n",
    "    if force_cpu:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"Принудительно используется CPU\")\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    sequence_length = 50\n",
    "    # Меньше тестов для CPU (они намного медленнее)\n",
    "    num_trials = 10 if device.type == 'cpu' else 1000\n",
    "    \n",
    "    # Информация об устройстве\n",
    "    if device.type == 'cuda':\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3) # GB\n",
    "        device_info = f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\"\n",
    "    else:\n",
    "        try:\n",
    "            import psutil\n",
    "            cpu_cores = psutil.cpu_count()\n",
    "            device_info = f\"CPU: {cpu_cores} cores\"\n",
    "        except:\n",
    "            device_info = \"CPU\"\n",
    "    \n",
    "    print(f\"Устройство: {device_info}\")\n",
    "    print(\"Загрузка модели.\")\n",
    "    model = YOLOBackboneConvLSTM(\n",
    "        yolo_ckpt=\"/workspace/yolo11m.pt\",\n",
    "        hidden_dim=256,\n",
    "        num_layers=1,\n",
    "        sequence_length=50,\n",
    "        img_size=256\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Подготовка данных\n",
    "    batch_input = torch.randn(1, sequence_length, 3, 256, 256, device=device)\n",
    "    \n",
    "    # Прогрев зависит от устройства\n",
    "    if device.type == 'cuda':\n",
    "        print(\"Прогрев GPU.\")\n",
    "        warmup_iterations = 10\n",
    "    else:\n",
    "        print(\"Минимальная инициализация CPU.\")\n",
    "        warmup_iterations = 2\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_iterations):\n",
    "            _ = model(batch_input)\n",
    "    \n",
    "    # Синхронизация после прогрева\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Измерения\n",
    "    print(f\"Измерение производительности ({num_trials} тестов)...\")\n",
    "    times = []\n",
    "\n",
    "    # Отключаем автоград\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_trials):\n",
    "            # Показываем прогресс для CPU (тесты медленные)\n",
    "            if device.type == 'cpu':\n",
    "                print(f\"  Тест {i+1}/{num_trials}...\")\n",
    "            \n",
    "            # Синхронизация с GPU перед началом\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.perf_counter()\n",
    "            output = model(batch_input)\n",
    "            \n",
    "            # Синхронизация с GPU после завершения\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            inference_time = (end_time - start_time) * 1000\n",
    "            times.append(inference_time)\n",
    "            \n",
    "            # Показываем время для CPU\n",
    "            if device.type == 'cpu':\n",
    "                print(f\"    Время: {inference_time:.1f} мс\")\n",
    "    \n",
    "    # Основные метрики\n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    sequence_fps = 1000 / mean_time\n",
    "    frame_throughput = sequence_length / (mean_time / 1000)\n",
    "    \n",
    "    return {\n",
    "        'device_info': device_info,\n",
    "        'latency_ms': mean_time,\n",
    "        'sequence_fps': sequence_fps,\n",
    "        'throughput_fps': frame_throughput\n",
    "    }\n",
    "\n",
    "\n",
    "def verify_measurement_correctness():\n",
    "    \"\"\"Проверка корректности измерений\"\"\"\n",
    "    print(\"\\n1. torch.no_grad() - отключение автограда\")\n",
    "    print(\"2. model.eval() - отключение dropout/batchnorm\")  \n",
    "    print(\"3. torch.cuda.synchronize() - синхронизация GPU\")\n",
    "    print(\"4. Прогрев GPU перед измерениями\")\n",
    "    print(\"5. Множественные измерения для статистики\")\n",
    "    print(\"6. perf_counter() - точное измерение времени\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_gpu = None\n",
    "    results_cpu = None\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nPhase 1: GPU Оценка производительности\")\n",
    "        print(\"-\" * 40)\n",
    "        results_gpu = benchmark(force_cpu=False)\n",
    "        \n",
    "        print(f\"\\nGPU Результаты:\")\n",
    "        print(f\"  Устройство: {results_gpu['device_info']}\")\n",
    "        print(f\"  Inference Latency: {results_gpu['latency_ms']:.1f} ms\")\n",
    "        print(f\"  Sequence Processing Rate: {results_gpu['sequence_fps']:.1f} sequences/sec\")\n",
    "        print(f\"  Frame Throughput: {results_gpu['throughput_fps']:.0f} frames/sec\")\n",
    "    else:\n",
    "        print(\"\\nGPU недоступен.\")\n",
    "    \n",
    "    print(f\"\\nPhase 2: CPU Оценка производительности\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"NOTE: CPU benchmark может занять несколько минут.\")\n",
    "    \n",
    "    user_input = input(\"Продолжить с CPU? (y/n): \")\n",
    "    if user_input.lower() == 'y':\n",
    "        results_cpu = benchmark(force_cpu=True)\n",
    "        \n",
    "        print(f\"\\nCPU Результаты:\")\n",
    "        print(f\"  Устройство: {results_cpu['device_info']}\")\n",
    "        print(f\"  Inference Latency: {results_cpu['latency_ms']:.1f} ms\")\n",
    "        print(f\"  Sequence Processing Rate: {results_cpu['sequence_fps']:.1f} sequences/sec\")\n",
    "        print(f\"  Frame Throughput: {results_cpu['throughput_fps']:.0f} frames/sec\")\n",
    "    else:\n",
    "        print(\"CPU benchmark пропущен.\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"Сравнение производительности.\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if results_gpu and results_cpu:\n",
    "        \n",
    "        print(f\"{'Метрики':<25} {'GPU':<15} {'CPU':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Inference Latency (ms)':<25} {results_gpu['latency_ms']:<15.1f} {results_cpu['latency_ms']:<15.1f}\")\n",
    "        print(f\"{'Sequence FPS':<25} {results_gpu['sequence_fps']:<15.1f} {results_cpu['sequence_fps']:<15.1f}\")\n",
    "        print(f\"{'Frame Throughput':<25} {results_gpu['throughput_fps']:<15.0f} {results_cpu['throughput_fps']:<15.0f}\")\n",
    "    \n",
    "    verify_measurement_correctness() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada470b-5444-460a-8b61-2e02e90a8d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
